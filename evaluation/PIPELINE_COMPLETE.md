# Automated Evaluation Pipeline - COMPLETE! âœ…

## What Was Built

You now have a **fully automated evaluation system** that:

1. âœ… **Loads test cases** from JSON (converted from CSV)
2. âœ… **Simulates agent responses** using Gemini API (matches React component behavior)
3. âœ… **Evaluates responses** using LLM-as-a-Judge (Gemini 2.5 Flash - FREE)
4. âœ… **Saves results** to CSV files (full results + summary)
5. âœ… **Generates statistics** (pass rates, category breakdown, average scores)

## Files Created

### Core Pipeline Files
- **`agent_simulator.py`** - Simulates negotiation chat agent
- **`run_evaluation.py`** - Main evaluation pipeline runner
- **`convert_csv_to_json.py`** - Converts CSV test cases to JSON (updated)

### Documentation
- **`AUTOMATION_GUIDE.md`** - Complete usage guide
- **`PIPELINE_COMPLETE.md`** - This file

## How It Works

```
Test Cases (JSON)
    â†“
For each test case:
    â”œâ”€â†’ Agent Simulator (Gemini API)
    â”‚   â””â”€â†’ Generates agent response
    â”‚
    â”œâ”€â†’ Parse Deal Updates
    â”‚   â””â”€â†’ Extract [NEW_PRICE], [NEW_QUANTITY], [NEW_OFFER]
    â”‚
    â”œâ”€â†’ Judge Evaluation (Gemini 2.5 Flash)
    â”‚   â””â”€â†’ Scores: Brand Voice, Negotiation, Deal Structure, Response Quality
    â”‚
    â””â”€â†’ Save Results
        â””â”€â†’ CSV files (full + summary)
```

## Quick Test Results

âœ… **Test Run Successful!**
- Test Case: TC-001 (Legal Sea Foods)
- Score: 4.25/5.0
- Status: PASSED
- Time: ~19 seconds per test case

## Usage Examples

### Run All Test Cases
```bash
cd evaluation
python run_evaluation.py
```

### Run Limited Test Cases (for testing)
```bash
python run_evaluation.py --max-cases 5
```

### Use Different Judge Model
```bash
# Use GPT-4 (requires OPENAI_API_KEY)
python run_evaluation.py --judge-model gpt-4

# Use Claude (requires ANTHROPIC_API_KEY)
python run_evaluation.py --judge-model claude-sonnet-4-20250514
```

## Output Files

Results are saved to `evaluation/results/`:

1. **`results_full_TIMESTAMP.csv`**
   - Complete evaluation results
   - All scores, reasoning, responses
   - One row per test case

2. **`results_summary_TIMESTAMP.csv`**
   - Summary statistics
   - Pass/fail counts
   - Category breakdown
   - Average scores

## Performance

- **Per Test Case**: ~15-35 seconds
- **20 Test Cases**: ~5-12 minutes
- **100 Test Cases**: ~25-60 minutes

## Cost

**Using Gemini (FREE):**
- Agent: FREE (gemini-3-flash-preview)
- Judge: FREE (gemini-2.5-flash)
- **Total: $0** ðŸŽ‰

## What's Next?

1. âœ… **Run full evaluation**: `python run_evaluation.py`
2. âœ… **Review results**: Open CSV files in Excel/Sheets
3. âœ… **Analyze failures**: Check which test cases failed
4. âœ… **Iterate**: Update agent prompts based on results
5. âœ… **Re-run**: Test improvements

## Key Features

- âœ… **Automated**: Runs all test cases without manual intervention
- âœ… **FREE**: Uses Gemini API (no cost for testing)
- âœ… **Comprehensive**: Evaluates brand voice, negotiation, deal structure, quality
- âœ… **Detailed**: Full reasoning and scores for each test case
- âœ… **Flexible**: Can use different judge models (GPT-4, Claude, Gemini)
- âœ… **Progress Tracking**: Shows progress bar and status updates
- âœ… **Error Handling**: Continues even if individual test cases fail

## Success Metrics

After running full evaluation, you'll see:
- Total test cases run
- Pass/fail counts and percentages
- Average scores by category
- Overall average score
- Failed cases (if any)

## Troubleshooting

See `AUTOMATION_GUIDE.md` for detailed troubleshooting steps.

## Summary

ðŸŽ‰ **Your automated evaluation pipeline is complete and working!**

You can now:
- Run evaluations automatically
- Get detailed scores and reasoning
- Track improvements over time
- Test agent changes systematically

**Next Step**: Run `python run_evaluation.py` to evaluate all your test cases!
